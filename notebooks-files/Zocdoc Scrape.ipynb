{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a266d106",
   "metadata": {},
   "source": [
    "# Scraping Zocdoc.com\n",
    "\n",
    "This notebook defines a function to scrape all the primary care physicians (PCPs) from zocdoc.com that turn up within a 1-mile radius when a search for a given zip code is conducted. \n",
    "\n",
    "The information scraped for each doctor includes:\n",
    "* Name\n",
    "* Specialty\n",
    "* Address\n",
    "* Upcoming appointments for the next five days, or soonest available appointment if they don't have any within five days\n",
    "* Rating and total number of reviews\n",
    "* In-network insurances\n",
    "* Education\n",
    "* Languages spoken at their office\n",
    "* Gender\n",
    "* NPI number (National Provider Identifier)\n",
    "\n",
    "The scape function does scrape reviews of the doctor. \n",
    "\n",
    "This notebook also contains a function to clean and standardize that data and output it to a pandas dataframe and csv \n",
    "\n",
    "Lastly, it will run the scraper on each zip code in NYC to produce a comprehensive dataset of doctors on zocdoc.com and their upcoming availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "20b012e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c0fb22e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 96.0.4664\n",
      "Get LATEST chromedriver version for 96.0.4664 google-chrome\n",
      "Driver [/Users/jmingram/.wdm/drivers/chromedriver/mac64/96.0.4664.45/chromedriver] found in cache\n",
      "/var/folders/tx/ryc062z51z1287rjpvrp4jlc0000gn/T/ipykernel_80875/1503906442.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16562661",
   "metadata": {},
   "source": [
    "A function to navigate to, pull up and scrape the information from the popup containing insurance information on individual doctor profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2ed4a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_insurance():\n",
    "    try:\n",
    "        scroll = driver.find_element(By.ID, 'insurance-target-element')\n",
    "    except: return []\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView(true);\", scroll)\n",
    "    time.sleep(2)\n",
    "    button = driver.find_element(By.ID, 'insurance-target-element').find_element(By.TAG_NAME, 'a')\n",
    "    button.click()\n",
    "    driver.switch_to.active_element\n",
    "    insurance_soup = BeautifulSoup(driver.page_source)\n",
    "    insurances = insurance_soup.find_all(class_='dx0sxs-0 nCTkH')\n",
    "    insurance_list = []\n",
    "    for insurance in insurances:\n",
    "        insurance_list.append(insurance.text)\n",
    "    return insurance_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9267f8",
   "metadata": {},
   "source": [
    "A function to check for duplicates if multiple zip codes are being scraped, in order to speed up the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8016262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_dupes(df_list, column, name):\n",
    "    for df in df_list:\n",
    "        if name in df[column].tolist():\n",
    "            print(f'HERE: {name}')\n",
    "            return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6a3d0c",
   "metadata": {},
   "source": [
    "The scrape function, which takes a given zip code and scrapes information on all doctors within a one mile radius of that zip code. If multi_zip evaluates to True, it will run the check_for_dupes function above to make sure it isn't scraping redundant information. For a wholistic scrape of a given zip code, multi_zip must be set to False. This function returns a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f8c72ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(zip_code, multi_zip):\n",
    "    print(f'Starting scrape at {datetime.now()}')\n",
    "    driver.maximize_window()\n",
    "    driver.get(f'https://www.zocdoc.com/search?address={zip_code}&after_5pm=false&before_10am=false&city=New+York&day_filter=AnyDay&dr_specialty=153&filters=%7B%22distance_radius%22%3A%5B%22to_1_mile%22%5D%7D&gender=-1&insurance_carrier=-1&insurance_plan=-1&language=-1&locationType=placemark&offset=0&reason_visit=75&searchQueryGuid=5bbddc60-6399-4dba-8faa-d624ed6c6018&searchType=specialty&search_query=Primary+Care+Physician+%28PCP%29&sort_type=Default&timesgridType=fiveDays&visitType=inPersonVisit')\n",
    "    soup_doc = BeautifulSoup(driver.page_source)\n",
    "    results = []\n",
    "    doc_list = []\n",
    "    pages = soup_doc.find(attrs={'data-test': 'search-pagination'}).find_all('a')\n",
    "    print(f\"Found {len(pages)} pages\")\n",
    "    #Iterate through all search result pages\n",
    "    for i, page in enumerate(pages):\n",
    "        print(f\"On page {i + 1}\")\n",
    "        #Iterate through all search results\n",
    "        search_results = soup_doc.find_all(attrs={'data-test': 'search-result-item'})\n",
    "        print(f\"Found {len(search_results)} search results\")\n",
    "        for card in search_results:\n",
    "            if (card.find(attrs={'data-test': 'doctor-card-info-name'}).text) in doc_list:\n",
    "                print(f\"SKIPPING: {card.find(attrs={'data-test': 'doctor-card-info-name'}).text}\")\n",
    "                continue\n",
    "            elif multi_zip:\n",
    "                if check_for_dupes(all_zips_dfs, 'name', card.find(attrs={'data-test': 'doctor-card-info-name'}).text) == True:\n",
    "                    continue \n",
    "            doc = {}\n",
    "            doc['name'] = card.find(attrs={'data-test': 'doctor-card-info-name'}).text\n",
    "            print(doc['name'])\n",
    "            doc_list.append(doc['name'])\n",
    "            doc['specialty'] = card.find(attrs={'data-test': 'doctor-card-info-specialty'}).text\n",
    "            if card.find(attrs={'data-test': 'doctor-card-info-location-address'}) is not None:\n",
    "                #Address doesn't appear for telehealth only providers\n",
    "                doc['street_address'] = card.find(attrs={'data-test': 'doctor-card-info-location-address'}).text\n",
    "                doc['city'] = card.find(attrs={'data-test': 'doctor-card-info-location-city'}).text\n",
    "                doc['state'] = card.find(attrs={'data-test': 'doctor-card-info-location-state'}).text\n",
    "                doc['zip'] = card.find(attrs={'data-test': 'doctor-card-info-location-zip'}).text\n",
    "            if card.find(attrs={'data-test': 'doctor-card-info-rating-number'}) is not None:\n",
    "                doc['rating'] = card.find(attrs={'data-test': 'doctor-card-info-rating-number'}).text \n",
    "                doc['num_reviews'] = card.find(attrs={'data-test': 'doctor-card-review-count'}).text[1:-1]\n",
    "            else:\n",
    "                doc['rating'] = np.nan\n",
    "                doc['num_reviews'] = '0'\n",
    "            if card.find(attrs={'data-test': 'no-availability-view'}) is not None:\n",
    "                doc['num_appts_next_5days'] = 0\n",
    "            else:\n",
    "                for grid in card.find_all(attrs={'data-test': 'timesgrid-day-column'}):\n",
    "                    if grid.find('a') is not None:\n",
    "                        doc['next_appt'] = grid.find('a')['aria-label']\n",
    "                        doc['num_appts_next_5days'] = len(grid.find_all('a'))\n",
    "            if card.find_all(attrs={'data-is-sponsored-result': 'true'}): \n",
    "                doc['sponsored'] = True\n",
    "            else:\n",
    "                doc['sponsored'] = False\n",
    "            doc['profile_url'] = card.find('a')['href']\n",
    "            #Move to individual provider profile\n",
    "            driver.get('http://zocdoc.com' + card.find('a')['href'])\n",
    "            doctor_soup = BeautifulSoup(driver.page_source)\n",
    "            educ = []\n",
    "            for item in doctor_soup.find_all(attrs={'data-test': 'education-list'}):\n",
    "                educ.append(item.text)\n",
    "            doc['education'] = educ \n",
    "            langs = []\n",
    "            if doctor_soup.find(attrs={'data-test': 'Languages-section'}) is not None:\n",
    "                for item in doctor_soup.find(attrs={'data-test': 'Languages-section'}).find_all('li'):\n",
    "                    langs.append(item.text)\n",
    "            doc['languages'] = langs\n",
    "            doc['gender'] = doctor_soup.find(attrs={'data-test': 'Sex-section'}).find('p').text\n",
    "            if doctor_soup.find(attrs={'itemprop': 'identifier'}) is not None:\n",
    "                doc['npi'] = doctor_soup.find(attrs={'itemprop': 'identifier'}).text #dtype string\n",
    "            if doc['num_appts_next_5days'] == 0:\n",
    "                try:\n",
    "                    doc['next_appt'] = doctor_soup.find(attrs={'role': 'gridcell'})['aria-label']\n",
    "                except:\n",
    "                    doc['next_appt'] = np.nan\n",
    "            try:\n",
    "                doc['insurance'] = get_insurance()\n",
    "            except:\n",
    "                print(\"error gathering insurance data\")\n",
    "            doc['asof'] = datetime.now()\n",
    "            results.append(doc)\n",
    "            #Return to search results page\n",
    "            driver.get('http://zocdoc.com' + page['href'])\n",
    "            soup_doc = BeautifulSoup(driver.page_source)\n",
    "        #Move to next page\n",
    "        page_nav = driver.find_element(By.TAG_NAME, 'nav').find_elements(By.TAG_NAME, 'span')\n",
    "        if i + 1 < len(page_nav):\n",
    "            page_nav[i+1].find_element(By.TAG_NAME, 'a').click()\n",
    "            soup_doc = BeautifulSoup(driver.page_source)\n",
    "    print(f'Finished scrape at {datetime.now()}')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f1c91",
   "metadata": {},
   "source": [
    "A function to turn the output of scrape, a list of dictionaries, into a dataframe with the appropriate data types, as well as save it to a csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb7e6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(results, csv_name):\n",
    "    df = pd.DataFrame(results)\n",
    "    df.drop_duplicates(subset='npi', inplace=True)\n",
    "    df.npi = df.npi.astype(int, errors='ignore')\n",
    "    df.rating = df.rating.astype(float, errors='ignore') \n",
    "    df.num_reviews = df.num_reviews.str.replace(',', '')\n",
    "    df.num_reviews = df.num_reviews.str.split(' ').str[0]\n",
    "    df.num_reviews = df.num_reviews.fillna(0).astype(int) \n",
    "    df.next_appt = pd.to_datetime(df.next_appt, format='%I:%M %p on %A, %B %d, %Y', errors='ignore')\n",
    "    df.to_csv(csv_name, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2caa1a",
   "metadata": {},
   "source": [
    "A faster scrape to get updated appointment availability. Requires a dataframe from the scrape and clean_data functions The code from this scrape is largely borrowed from scrape fumction, it just searches for doctors in the dataframe and updates the next_appt and num_appts_next_5days with new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "78ae0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_availability_scrape(frame_to_update):\n",
    "    print(f'Scrape began at {datetime.now()}')\n",
    "    driver.maximize_window()\n",
    "    zips = set(list(frame_to_update.zip))\n",
    "    updated_list = []\n",
    "    frame_to_update['asof'] = ''\n",
    "    for zip_code in zips:\n",
    "        print(f'Starting zip code: {zip_code}')\n",
    "        driver.get(f'https://www.zocdoc.com/search?address={zip_code}&after_5pm=false&before_10am=false&city=New+York&day_filter=AnyDay&dr_specialty=153&filters=%7B%22distance_radius%22%3A%5B%22to_1_mile%22%5D%7D&gender=-1&insurance_carrier=-1&insurance_plan=-1&language=-1&locationType=placemark&offset=0&reason_visit=75&searchQueryGuid=5bbddc60-6399-4dba-8faa-d624ed6c6018&searchType=specialty&search_query=Primary+Care+Physician+%28PCP%29&sort_type=Default&timesgridType=fiveDays&visitType=inPersonVisit')\n",
    "        soup_doc = BeautifulSoup(driver.page_source)\n",
    "        pages = soup_doc.find(attrs={'data-test': 'search-pagination'}).find_all('a')\n",
    "        for i, page in enumerate(pages):\n",
    "            search_results = soup_doc.find_all(attrs={'data-test': 'search-result-item'})\n",
    "            for card in search_results:\n",
    "                name = card.find(attrs={'data-test': 'doctor-card-info-name'}).text\n",
    "                if name not in updated_list:\n",
    "                    print('Found a new name')\n",
    "                    if card.find(attrs={'data-test': 'no-availability-view'}) is not None:\n",
    "                        print('No upcoming availability, went into the page to get something else')\n",
    "                        frame_to_update.loc[frame_to_update.name == name, 'num_appts_next_5days'] = 0 \n",
    "                        time.sleep(0.5)\n",
    "                        driver.get('http://zocdoc.com' + card.find('a')['href'])\n",
    "                        doctor_soup = BeautifulSoup(driver.page_source)\n",
    "                        try:\n",
    "                            frame_to_update.loc[frame_to_update.name == name, 'next_appt'] = doctor_soup.find(attrs={'role': 'gridcell'})['aria-label'] #mod\n",
    "                        except:\n",
    "                            frame_to_update.loc[frame_to_update.name == name, 'next_appt'] = np.nan \n",
    "                    else:\n",
    "                        print('Updating upcoming availability')\n",
    "                        for grid in card.find_all(attrs={'data-test': 'timesgrid-day-column'}):\n",
    "                            if grid.find('a') is not None:\n",
    "                                frame_to_update.loc[frame_to_update.name == name, 'next_appt'] = grid.find('a')['aria-label']\n",
    "                                frame_to_update.loc[frame_to_update.name == name, 'num_appts_next_5days'] = len(grid.find_all('a'))\n",
    "                    updated_list.append(name)\n",
    "                    frame_to_update.loc[frame_to_update == name, 'asof'] = datetime.now()\n",
    "                else: print('Already updated this name')\n",
    "            time.sleep(1)\n",
    "    print(f'Scrape concluded at {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a9ef58",
   "metadata": {},
   "source": [
    "## Citywide Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf903b1",
   "metadata": {},
   "source": [
    "Getting the list of NYC zip codes\n",
    "\n",
    "Data source: https://www.unitedstateszipcodes.org/zip-code-database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b7301b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmingram/.pyenv/versions/3.10.0/lib/python3.10/site-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "full_df = pd.read_csv('zip_code_database.csv')\n",
    "nyc_counties = ['New York County', 'Queens County', 'Bronx County', 'Kings County', 'Richmond County']\n",
    "nyc_zips = full_df[(full_df.county.isin(nyc_counties) == True) & (full_df.decommissioned == 0) & (full_df.state == 'NY')]\n",
    "nyc_zips.zip = nyc_zips.zip.astype(str)\n",
    "zips = list(nyc_zips.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77771f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS WILL SCRAPE THE WHOLE CITY\n",
    "all_zips_dfs = []\n",
    "broken_zips = []\n",
    "for zip_code in zips:\n",
    "    try:\n",
    "        print(f'**STARTING {zip_code} SCRAPE**')\n",
    "        results = scrape(zip_code, True)\n",
    "        if len(results) > 0:\n",
    "            df = clean_data(results, f'{zip_code}_data.csv')\n",
    "            all_zips_dfs.append(df)\n",
    "            print(f'{zip_code} added to dataframes list')\n",
    "    except Exception as e:\n",
    "        broken_zips.append(zip_code)\n",
    "        print(f'{zip_code} scrape encountered an error')\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "e9348ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nyc_docs = pd.concat(all_zips_dfs)\n",
    "#nyc_docs.to_csv('all-zocdoc-data-nyc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "9b96d0b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(487, 16)"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyc_docs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
